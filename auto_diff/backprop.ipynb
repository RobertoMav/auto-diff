{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para rodar o MNIST, tens que rodar o arquivo MNIST.ipynb. Se não quiser pode rodar todas as células menos a 5 e a 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mnist1d.data import get_dataset_args, make_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(42)\n",
    "# matplotlib.use('TkAgg')\n",
    "\n",
    "\n",
    "class Activation:\n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        return np.where(z > 0, 1, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(z):\n",
    "        s = Activation.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(z):\n",
    "        return np.tanh(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh_derivative(z):\n",
    "        return 1 - np.tanh(z) ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    def LeakyReLU(z, alpha=0.01):\n",
    "        return np.maximum(alpha * z, z)\n",
    "\n",
    "    @staticmethod\n",
    "    def LeakyReLU_derivative(z, alpha=0.01):\n",
    "        return np.where(z > 0, 1, alpha)\n",
    "\n",
    "    @staticmethod\n",
    "    def linear(z):\n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_derivative(z):\n",
    "        return np.ones_like(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        # Ficou brabo, not stable\n",
    "        # (https://stackoverflow.com/questions/40575841/numpy-calculate-the-derivative-of-the-softmax-function)\n",
    "        # So we subtract the maximum value from the input to make it stable\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax_derivative(z):\n",
    "        # Compute softmax for z\n",
    "        # s = Activation.softmax(z)  # shape (batch_size, num_classes)\n",
    "        return 1\n",
    "\n",
    "\n",
    "class Loss:\n",
    "    @staticmethod\n",
    "    def MSE(y, y_hat):\n",
    "        return 1 / y.shape[0] * (y - y_hat).T @ (y - y_hat)\n",
    "\n",
    "    def MSE_derivative(y, y_hat):\n",
    "        return y_hat - y\n",
    "\n",
    "    @staticmethod\n",
    "    def BCE(y, y_hat):\n",
    "        return -1 / y.shape[0] * (y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "\n",
    "    def BCE_derivative(y, y_hat):\n",
    "        return (y_hat - y) / (y_hat * (1 - y_hat))\n",
    "\n",
    "    @staticmethod\n",
    "    def CrossEntropy(y, y_hat):\n",
    "        y_hat = np.clip(y_hat, 1e-15, 1 - 1e-15)\n",
    "        return -1 / y.shape[0] * np.sum(y * np.log(y_hat))\n",
    "\n",
    "    def CrossEntropy_derivative(y, y_hat):\n",
    "        return y_hat - y\n",
    "\n",
    "\n",
    "class Initialization:\n",
    "    @staticmethod\n",
    "    def xavier(size):\n",
    "        input = size[0]\n",
    "        stddev = np.sqrt(1 / input)\n",
    "        return np.random.normal(0, stddev, size=size)\n",
    "\n",
    "    @staticmethod\n",
    "    def he(size):\n",
    "        fan_in = size[0]\n",
    "        stddev = np.sqrt(2 / fan_in)\n",
    "        return np.random.normal(0, stddev, size=size)\n",
    "\n",
    "    @staticmethod\n",
    "    def normal(size):\n",
    "        return np.random.randn(*size)\n",
    "\n",
    "    @staticmethod\n",
    "    def uniform(size):\n",
    "        return np.random.uniform(size=size)\n",
    "\n",
    "    @staticmethod\n",
    "    def zeros(size):\n",
    "        return np.zeros(size)\n",
    "\n",
    "    @staticmethod\n",
    "    def explode(size):\n",
    "        return 1000000 * np.random.randn(*size)\n",
    "\n",
    "    @staticmethod\n",
    "    def vanish(size):\n",
    "        return np.random.normal(size=size) / 1000000\n",
    "\n",
    "\n",
    "class Optimizer:\n",
    "    def __init__(self, learning_rate, network):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.grads = network.grads\n",
    "        self.weights = network.weights\n",
    "        self.biases = network.biases\n",
    "\n",
    "    def zero_grad(self):\n",
    "        return {key: np.zeros_like(value) for key, value in self.grads.items()}\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, learning_rate, network):\n",
    "        super().__init__(learning_rate, network)\n",
    "\n",
    "    def step(self):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] = self.weights[i] - self.learning_rate * self.grads[f\"dw{i + 1}\"]\n",
    "            self.biases[i] = self.biases[i] - self.learning_rate * self.grads[f\"db{i + 1}\"]\n",
    "\n",
    "        return self.weights, self.biases\n",
    "\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, learning_rate, network, beta1=0.9, gamma=0.999, epsilon=1e-8):\n",
    "        super().__init__(learning_rate, network)\n",
    "        self.beta1 = beta1\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        # Initializing as zeros so that we can have momentum and RMSprop for each parameter\n",
    "        self.m_w = [np.zeros_like(w) for w in self.weights]\n",
    "        self.v_w = [np.zeros_like(w) for w in self.weights]\n",
    "        self.m_b = [np.zeros_like(b) for b in self.biases]\n",
    "        self.v_b = [np.zeros_like(b) for b in self.biases]\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        for i in range(len(self.weights)):\n",
    "            # Update for weights\n",
    "            self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * self.grads[f\"dw{i + 1}\"]\n",
    "            self.v_w[i] = self.gamma * self.v_w[i] + (1 - self.gamma) * np.square(\n",
    "                self.grads[f\"dw{i + 1}\"]\n",
    "            )\n",
    "            m_w_hat = self.m_w[i] / (1 - self.beta1**self.t)\n",
    "            v_w_hat = self.v_w[i] / (1 - self.gamma**self.t)\n",
    "            self.weights[i] = self.weights[i] - self.learning_rate * m_w_hat / (\n",
    "                np.sqrt(v_w_hat) + self.epsilon\n",
    "            )\n",
    "\n",
    "            # Update for biases\n",
    "            self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * self.grads[f\"db{i + 1}\"]\n",
    "            self.v_b[i] = self.gamma * self.v_b[i] + (1 - self.gamma) * np.square(\n",
    "                self.grads[f\"db{i + 1}\"]\n",
    "            )\n",
    "            m_b_hat = self.m_b[i] / (1 - self.beta1**self.t)\n",
    "            v_b_hat = self.v_b[i] / (1 - self.gamma**self.t)\n",
    "            self.biases[i] = self.biases[i] - self.learning_rate * m_b_hat / (\n",
    "                np.sqrt(v_b_hat) + self.epsilon\n",
    "            )\n",
    "\n",
    "        return self.weights, self.biases\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, neurons, activations, weight_initialization) -> None:\n",
    "        self.MLP_DEPTH = len(neurons) - 1\n",
    "        self.neurons = neurons\n",
    "        if len(activations) == 1:\n",
    "            self.activations = [activations[0] for _ in range(self.MLP_DEPTH)]\n",
    "        elif len(activations) == 2 and self.MLP_DEPTH > 2:\n",
    "            self.activations = [activations[0] for _ in range(self.MLP_DEPTH)]\n",
    "            self.activations[-1] = activations[1]\n",
    "        else:\n",
    "            self.activations = activations\n",
    "\n",
    "        self.activation_derivatives = [\n",
    "            getattr(Activation, f\"{activation.__name__}_derivative\")\n",
    "            for activation in self.activations\n",
    "        ]\n",
    "\n",
    "        if isinstance(weight_initialization, list):\n",
    "            # Direct weight assignment\n",
    "            self.weights = weight_initialization\n",
    "        else:\n",
    "            # Use initialization method\n",
    "            self.weights = [\n",
    "                np.squeeze(weight_initialization((neurons[i + 1], neurons[i])))\n",
    "                for i in range(self.MLP_DEPTH)\n",
    "            ]\n",
    "\n",
    "        self.biases = [np.zeros((neurons[i + 1])) for i in range(self.MLP_DEPTH)]\n",
    "\n",
    "        self.cache = {}\n",
    "        self.grads = {}\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.cache[\"a0\"] = a = input\n",
    "\n",
    "        for i in range(self.MLP_DEPTH):\n",
    "            z = a @ self.weights[i].T + self.biases[i]\n",
    "            a = self.activations[i](z)\n",
    "\n",
    "            self.cache[f\"w{i + 1}\"] = self.weights[i]\n",
    "            self.cache[f\"z{i + 1}\"] = z\n",
    "            self.cache[f\"a{i + 1}\"] = a\n",
    "\n",
    "        return a\n",
    "\n",
    "    def backward(self, y, loss):\n",
    "        BATCH_SIZE = y.shape[0]\n",
    "\n",
    "        self.loss_derivative = getattr(Loss, f\"{loss.__name__}_derivative\")\n",
    "\n",
    "        # Output layer\n",
    "        dz = self.loss_derivative(y, self.cache[f\"a{self.MLP_DEPTH}\"])\n",
    "        da = self.activation_derivatives[self.MLP_DEPTH - 1](self.cache[f\"z{self.MLP_DEPTH}\"])\n",
    "        dl = dz * da\n",
    "\n",
    "        self.grads[f\"dw{self.MLP_DEPTH}\"] = (\n",
    "            (1 / BATCH_SIZE) * dl.T @ self.cache[f\"a{self.MLP_DEPTH - 1}\"]\n",
    "        )\n",
    "        self.grads[f\"db{self.MLP_DEPTH}\"] = (1 / BATCH_SIZE) * np.sum(dz, axis=0, keepdims=True)\n",
    "\n",
    "        for i in range(self.MLP_DEPTH, 1, -1):\n",
    "            if len(dl.shape) == 1:\n",
    "                dl = dl.reshape(-1, 1)\n",
    "            if len(self.cache[f\"w{i}\"].shape) == 1:\n",
    "                self.cache[f\"w{i}\"] = self.cache[f\"w{i}\"].reshape(1, -1)\n",
    "\n",
    "            dz = dl @ self.cache[f\"w{i}\"]\n",
    "\n",
    "            dz *= self.activation_derivatives[i - 1](self.cache[f\"a{i - 1}\"])\n",
    "\n",
    "            self.grads[f\"dw{i - 1}\"] = (1 / BATCH_SIZE) * dz.T @ self.cache[f\"a{i - 2}\"]\n",
    "            self.grads[f\"db{i - 1}\"] = (1 / BATCH_SIZE) * np.sum(dz, axis=0, keepdims=True)\n",
    "            dl = dz\n",
    "\n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save the model weights and biases to a file.\"\"\"\n",
    "        model_data = {\n",
    "            \"weights\": self.weights,\n",
    "            \"biases\": self.biases,\n",
    "            \"neurons\": self.neurons,\n",
    "            \"activations\": [act.__name__ for act in self.activations],\n",
    "        }\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, filepath, weight_initialization):\n",
    "        \"\"\"Load a model from a file.\"\"\"\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            model_data = pickle.load(f)\n",
    "\n",
    "        # Recreate activation functions from their names\n",
    "        activations = [getattr(Activation, act_name) for act_name in model_data[\"activations\"]]\n",
    "\n",
    "        # Create a new instance of the model\n",
    "        model = cls(model_data[\"neurons\"], activations, weight_initialization)\n",
    "\n",
    "        # Load the weights and biases\n",
    "        model.weights = model_data[\"weights\"]\n",
    "        model.biases = model_data[\"biases\"]\n",
    "\n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_set,\n",
    "        train_labels,\n",
    "        test_set,\n",
    "        test_labels,\n",
    "        val_split=0.15,\n",
    "        num_classes=10,\n",
    "    ):\n",
    "        self.train_set = train_set\n",
    "        self.test_set = test_set\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # One-hot encode the labels\n",
    "        self.train_labels = self.one_hot_encode(train_labels)\n",
    "        self.test_labels = self.one_hot_encode(test_labels)\n",
    "\n",
    "        # Create validation set\n",
    "        val_size = int(len(train_set) * val_split)\n",
    "        self.val_set = train_set[-val_size:]\n",
    "        self.val_labels = self.train_labels[-val_size:]\n",
    "        self.train_set = train_set[:-val_size]\n",
    "        self.train_labels = self.train_labels[:-val_size]\n",
    "\n",
    "    def one_hot_encode(self, labels):\n",
    "        # Ensure labels is a 1D array\n",
    "        labels = np.array(labels).ravel()\n",
    "        # Check if labels are in the correct range\n",
    "        if np.min(labels) < 0 or np.max(labels) >= self.num_classes:\n",
    "            raise ValueError(f\"Labels must be in the range 0 to {self.num_classes - 1}\")\n",
    "        # Create one-hot encoded array\n",
    "        one_hot = np.zeros((labels.size, self.num_classes))\n",
    "        one_hot[np.arange(labels.size), labels] = 1\n",
    "        return one_hot\n",
    "\n",
    "    def create_batches(self, batch_size, dataset=\"train\"):\n",
    "        if dataset == \"train\":\n",
    "            data, labels = self.train_set, self.train_labels\n",
    "        elif dataset == \"val\":\n",
    "            data, labels = self.val_set, self.val_labels\n",
    "        elif dataset == \"test\":\n",
    "            data, labels = self.test_set, self.test_labels\n",
    "        else:\n",
    "            raise ValueError(\"dataset must be 'train', 'val', or 'test'\")\n",
    "\n",
    "        indices = np.arange(len(data))\n",
    "        np.random.shuffle(indices)\n",
    "        for start_idx in range(0, len(data), batch_size):\n",
    "            batch_indices = indices[start_idx : start_idx + batch_size]\n",
    "            yield data[batch_indices], labels[batch_indices]\n",
    "\n",
    "    def get_test_data(self):\n",
    "        return self.test_set, self.test_labels\n",
    "\n",
    "    def get_val_data(self):\n",
    "        return self.val_set, self.val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_plots_grad_actv(ax1, ax2, weight_grads, activations):\n",
    "    # Clear the previous plots\n",
    "    ax1.clear()\n",
    "    ax2.clear()\n",
    "\n",
    "    # --- Update Weight Histogram on ax1 ---\n",
    "    # Combine all layer weights into a single array for the histogram\n",
    "    # print(len(activations))\n",
    "    all_grads = np.concatenate([w.flatten() for w in weight_grads])\n",
    "    all_activations = np.concatenate([a.flatten() for a in activations])\n",
    "\n",
    "    # Plot histogram of all weights combined\n",
    "    ax1.hist(all_grads, bins=50, color=\"blue\", alpha=0.7)\n",
    "\n",
    "    # Add title and labels\n",
    "    ax1.set_title(\"Weight Grads Distribution\")\n",
    "    ax1.set_xlabel(\"Weight values\")\n",
    "    ax1.set_ylabel(\"Frequency\")\n",
    "\n",
    "    # --- Plot Loss over Epochs on ax2 ---\n",
    "    ax2.hist(all_activations, bins=50, color=\"red\", alpha=0.7)\n",
    "\n",
    "    # Add title and labels\n",
    "    ax2.set_title(\"Activations Distribution\")\n",
    "    ax2.set_xlabel(\"Activation values\")\n",
    "    ax2.set_ylabel(\"Frequency\")\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Draw the updated plots\n",
    "    fig = ax1.figure  # Get the figure object associated with the axes\n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "\n",
    "\n",
    "def update_plots_loss_acc(bx1, bx2, loss_history, acc_history, total_epochs):\n",
    "    # Clear the previous plots\n",
    "    bx1.clear()\n",
    "    bx2.clear()\n",
    "\n",
    "    # --- Plot Loss over Epochs on bx2 ---\n",
    "    bx1.plot(loss_history, color=\"red\")\n",
    "\n",
    "    # Add title and labels\n",
    "    bx1.set_title(\"Loss over Epochs\")\n",
    "    bx1.set_xlabel(\"Epoch\")\n",
    "    bx1.set_ylabel(\"Loss\")\n",
    "\n",
    "    bx1.set_xlim(0, total_epochs)  # Set x-axis limits to number of epochs\n",
    "\n",
    "    # --- Plot accuracy over Epochs on ax2 ---\n",
    "    bx2.plot(acc_history, color=\"red\")\n",
    "\n",
    "    # Add title and labels\n",
    "    bx2.set_title(\"Accuracy over Epochs in the Validation Data\")\n",
    "    bx2.set_xlabel(\"Epoch\")\n",
    "    bx2.set_ylabel(\"Accuracy\")\n",
    "\n",
    "    bx2.set_xlim(0, total_epochs)  # Set x-axis limits to number of epochs\n",
    "    bx2.set_ylim(0, 1)  # Set y-axis limits to 0-1\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Draw the updated plots\n",
    "    fig = bx1.figure  # Get the figure object associated with the axes\n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "\n",
    "    # Optional: Pause to allow the plot to update\n",
    "    # plt.pause(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_whole_set(model, test_data, test_labels, loss_fn):\n",
    "    # @model_eval\n",
    "    y_hat = model.forward(test_data)\n",
    "    total_loss = loss_fn(test_labels, y_hat)\n",
    "\n",
    "    predictions = np.argmax(y_hat, axis=1)\n",
    "    true_labels = np.argmax(test_labels, axis=1)\n",
    "    accuracy = np.mean(predictions == true_labels)\n",
    "\n",
    "    return accuracy, total_loss\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    dataset,\n",
    "    epochs,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    batch_size=32,\n",
    "    save_path=\"./models/best_model.pkl\",\n",
    "    plot=True,\n",
    "):\n",
    "    metrics = {}\n",
    "    metrics[\"train_loss\"] = []\n",
    "    metrics[\"val_loss\"] = []\n",
    "    metrics[\"val_acc\"] = []\n",
    "    metrics[\"network_grads\"] = []\n",
    "    metrics[\"network_activations\"] = []\n",
    "    metrics[\"train_time\"] = []\n",
    "    best_val_acc = 0\n",
    "\n",
    "    if plot:\n",
    "        # Set up the plot outside the loop with three subplots\n",
    "        plt.ion()  # Turn on interactive mode for live updating plots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        fig1, (bx1, bx2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        batch_loss = 0\n",
    "\n",
    "        for X_batch, y_batch in dataset.create_batches(batch_size, \"train\"):\n",
    "            start = time.time()\n",
    "            pred = model.forward(X_batch)\n",
    "            batch_loss += loss_fn(y_batch, pred)\n",
    "            model.backward(y_batch, loss_fn)\n",
    "            model.weights, model.biases = optimizer.step()\n",
    "            metrics[\"network_grads\"].append([v for v in model.grads.values()])\n",
    "            metrics[\"network_activations\"].append(\n",
    "                [v for k, v in model.cache.items() if \"a\" in k and \"0\" not in k]\n",
    "            )\n",
    "            if plot:\n",
    "                update_plots_grad_actv(\n",
    "                    ax1,\n",
    "                    ax2,\n",
    "                    metrics[\"network_grads\"][-1],\n",
    "                    metrics[\"network_activations\"][-1],\n",
    "                )\n",
    "\n",
    "        metrics[\"train_time\"].append(time.time() - start)\n",
    "        # Validate on validation set\n",
    "        val_acc, val_loss = test_whole_set(model, *dataset.get_val_data(), loss_fn)\n",
    "        metrics[\"val_loss\"].append(val_loss)\n",
    "        metrics[\"val_acc\"].append(val_acc)\n",
    "        metrics[\"train_loss\"].append(batch_loss / (len(dataset.train_set) // batch_size))\n",
    "        if plot:\n",
    "            update_plots_loss_acc(bx1, bx2, metrics[\"train_loss\"], metrics[\"val_acc\"], epochs - 1)\n",
    "        print(\n",
    "            f\"Epoch {epoch} Train Loss: {metrics['train_loss'][-1]:.4f}, Val Acc: {val_acc:.4f}, Val Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Save the best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            model.save(save_path)\n",
    "            print(f\"New best model saved with validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_data.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_set \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_data.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_labels.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m test_set \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_data.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Developer/Study/backprop/bento_luciano_roberto/.venv/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:455\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 455\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    456\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_data.npy'"
     ]
    }
   ],
   "source": [
    "train_set = np.load(\"train_data.npy\")\n",
    "train_labels = np.load(\"train_labels.npy\")\n",
    "test_set = np.load(\"test_data.npy\")\n",
    "test_labels = np.load(\"test_labels.npy\")\n",
    "\n",
    "dataset = Dataset(train_set, train_labels, test_set, test_labels)\n",
    "\n",
    "defaults = get_dataset_args()\n",
    "data = make_dataset(defaults)\n",
    "\n",
    "X_train, y_train = data[\"x\"], data[\"y\"].reshape(-1, 1)\n",
    "X_val, y_val = data[\"x_test\"], data[\"y_test\"].reshape(-1, 1)\n",
    "\n",
    "\n",
    "mnist_1d = Dataset(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:03<00:15,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train Loss: 2.6343, Val Acc: 0.9006, Val Loss: 1.5518\n",
      "Model saved to ./models/best_model.pkl\n",
      "New best model saved with validation accuracy: 0.9006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:07<00:10,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train Loss: 1.2271, Val Acc: 0.8963, Val Loss: 0.8487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:11<00:07,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train Loss: 0.6356, Val Acc: 0.9141, Val Loss: 0.5030\n",
      "Model saved to ./models/best_model.pkl\n",
      "New best model saved with validation accuracy: 0.9141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:15<00:04,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Train Loss: 0.4925, Val Acc: 0.9006, Val Loss: 0.5398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:21<00:00,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Train Loss: 0.4703, Val Acc: 0.9043, Val Loss: 0.4095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 64\n",
    "loss_fn = Loss.CrossEntropy\n",
    "MODEL = MLP([784, 512, 10], [Activation.relu, Activation.softmax], Initialization.he)\n",
    "optim = Adam(4e-3, MODEL)\n",
    "training_metrics = train(MODEL, dataset, EPOCHS, optim, loss_fn, BATCH_SIZE, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:08<00:08,  8.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train Loss: 2.1075, Val Acc: 0.3550, Val Loss: 1.6645\n",
      "Model saved to ./models/best_model.pkl\n",
      "New best model saved with validation accuracy: 0.3550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:15<00:00,  7.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train Loss: 1.4582, Val Acc: 0.4300, Val Loss: 1.4615\n",
      "Model saved to ./models/best_model.pkl\n",
      "New best model saved with validation accuracy: 0.4300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "BATCH_SIZE = 64\n",
    "loss_fn = Loss.CrossEntropy\n",
    "MODEL = MLP(\n",
    "    [40, 512, 256, 128, 127, 10],\n",
    "    [Activation.relu, Activation.softmax],\n",
    "    Initialization.he,\n",
    ")\n",
    "optim = Adam(1e-3, MODEL)\n",
    "training_metrics = train(MODEL, mnist_1d, EPOCHS, optim, loss_fn, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from models/best_model.pkl\n",
      "Test Accuracy: 0.4490, Test Loss: 1.4374\n"
     ]
    }
   ],
   "source": [
    "loaded_model = MLP.load(\"models/best_model.pkl\", Initialization.he)\n",
    "\n",
    "test_acc, test_loss = test_whole_set(loaded_model, *mnist_1d.get_test_data(), loss_fn)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
