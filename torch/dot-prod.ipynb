{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "tensor([[[-0.7354, -0.2506,  1.0311,  0.8589],\n",
      "         [ 0.3456, -0.7522, -1.0590, -0.3405],\n",
      "         [-1.5162, -0.8362,  1.9151,  1.7940]],\n",
      "\n",
      "        [[-1.4994, -1.6560, -3.3302, -2.9880],\n",
      "         [ 0.5085,  0.1275, -0.0325,  0.0881],\n",
      "         [ 0.3778,  1.2821,  3.1539,  2.5964]]])\n",
      "torch.Size([2, 3, 4])\n",
      "tensor([[[-0.7354, -0.2506,  1.0311,  0.8589],\n",
      "         [ 0.3456, -0.7522, -1.0590, -0.3405],\n",
      "         [-1.5162, -0.8362,  1.9151,  1.7940]],\n",
      "\n",
      "        [[-1.4994, -1.6560, -3.3302, -2.9880],\n",
      "         [ 0.5085,  0.1275, -0.0325,  0.0881],\n",
      "         [ 0.3778,  1.2821,  3.1539,  2.5964]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a tensor of given shape\n",
    "x = torch.randn(2, 3, 2)\n",
    "\n",
    "# Create a tensor of given shape\n",
    "y = torch.randn(2, 2, 4)\n",
    "\n",
    "z = x @ y\n",
    "\n",
    "print(z.shape)\n",
    "print(z)\n",
    "\n",
    "# Calculate the dot product of x and y\n",
    "z = torch.bmm(x, y)\n",
    "print(z.shape)\n",
    "\n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalized Dot Product (GDT): 10.08789348602295\n",
      "Generalized Dot Product (GDT): 10.08789348602295\n"
     ]
    }
   ],
   "source": [
    "# Create two tensors of the same shape (a, b, c)\n",
    "a, b, c = 2, 3, 4  # Example dimensions\n",
    "X = torch.randn(a, b, c)\n",
    "Y = torch.randn(a, b, c)\n",
    "\n",
    "# Compute Hadamard (element-wise) product\n",
    "hadamard_product = X * Y\n",
    "\n",
    "# Sum over all elements (generalized dot product)\n",
    "GDT = hadamard_product.sum()\n",
    "\n",
    "print(\"Generalized Dot Product (GDT):\", GDT.item())  # Convert to scalar\n",
    "\n",
    "GDT = torch.einsum('ijk,ijk->', X, Y)  # Summing over all elements\n",
    "print(\"Generalized Dot Product (GDT):\", GDT.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.49949900000000014\n",
      "-0.49950000000000006\n"
     ]
    }
   ],
   "source": [
    "f = lambda x: x**2 - 1.5*x\n",
    "df = lambda x: 2*x - 1.5\n",
    "\n",
    "x = 1\n",
    "def f_lin(h):\n",
    "    # print(h, x)\n",
    "    # print(f(x))\n",
    "    # print(df(x))\n",
    "    # print(h-x)\n",
    "    return f(x) + df(x)*(h-x)\n",
    "\n",
    "print(f(x + 0.001))\n",
    "print(f_lin(x + 0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "-0.505\n",
      "-0.0050000000000000044\n",
      "-0.007574999999999994\n",
      "-0.012575\n",
      "0.003769130624999995\n",
      "-0.008805869375000004\n",
      "-0.0018897699895504324\n",
      "-0.010695639364550437\n",
      "0.0009439429609511673\n",
      "-0.00975169640359927\n",
      "-0.00047239373992511054\n",
      "-0.01022409014352438\n",
      "0.00023618543167899277\n",
      "-0.009987904711845387\n",
      "-0.00011814278593592561\n",
      "-0.010106047497781313\n",
      "5.908249274375402e-05\n",
      "-0.010046965005037559\n",
      "-2.9550286731964276e-05\n"
     ]
    }
   ],
   "source": [
    "x = 0.5\n",
    "for i in range(10):\n",
    "    h = 0.01    \n",
    "    print(x)\n",
    "    deriv = f_lin(x + h)\n",
    "    print(deriv)\n",
    "    x += deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List\n",
    "import numpy as np\n",
    "import torch\n",
    "import jax\n",
    "\n",
    "from numpy.typing import NDArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running a simple SDG in pytorch\n",
    "\n",
    "def ls_sdg(\n",
    "        X: NDArray,\n",
    "        y: NDArray,\n",
    "        parameters: Dict[str, Any]\n",
    ") -> NDArray:\n",
    "    \n",
    "    epochs: int = parameters.get(\"epochs\")\n",
    "    learning_rate: float = parameters.get(\"lr\")\n",
    "    W: NDArray = parameters.get(\"parameters\") \n",
    "    # For loop - can we do this in a Tensor\n",
    "    for epoch in range(epochs):\n",
    "        # Compute forward\n",
    "        pred: NDArray = W @ X.T\n",
    "        loss: float = ((y - pred)**2).mean()\n",
    "        print(f\"Epoch {epoch} - Loss: {loss}\")\n",
    "        # Compute backward\n",
    "        grad: NDArray = -2 * (y - pred) @ X\n",
    "        W -= learning_rate * grad\n",
    "    return W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n",
      "(100,)\n",
      "Epoch 0 - Loss: 3.5335997774653216\n",
      "Epoch 1 - Loss: 4.6450198025405545\n",
      "Epoch 2 - Loss: 6.5141812208452246\n",
      "Epoch 3 - Loss: 9.574774509849354\n",
      "Epoch 4 - Loss: 14.517244618931322\n",
      "Epoch 5 - Loss: 22.43974930029912\n",
      "Epoch 6 - Loss: 35.087997090289804\n",
      "Epoch 7 - Loss: 55.23627976834179\n",
      "Epoch 8 - Loss: 87.2928085472242\n",
      "Epoch 9 - Loss: 138.26138419313384\n",
      "Epoch 10 - Loss: 219.26914857320605\n",
      "Epoch 11 - Loss: 347.99362921131245\n",
      "Epoch 12 - Loss: 552.5184272797243\n",
      "Epoch 13 - Loss: 877.4584916862449\n",
      "Epoch 14 - Loss: 1393.69092697865\n",
      "Epoch 15 - Loss: 2213.8136631616408\n",
      "Epoch 16 - Loss: 3516.70358375056\n",
      "Epoch 17 - Loss: 5586.5303346164865\n",
      "Epoch 18 - Loss: 8874.734874930784\n",
      "Epoch 19 - Loss: 14098.490528907321\n",
      "Epoch 20 - Loss: 22397.121269004834\n",
      "Epoch 21 - Loss: 35580.59265361947\n",
      "Epoch 22 - Loss: 56524.27223812105\n",
      "Epoch 23 - Loss: 89796.05820612764\n",
      "Epoch 24 - Loss: 142652.65535993912\n",
      "Epoch 25 - Loss: 226622.29005120872\n",
      "Epoch 26 - Loss: 360019.06019489886\n",
      "Epoch 27 - Loss: 571937.3096624913\n",
      "Epoch 28 - Loss: 908597.2152933413\n",
      "Epoch 29 - Loss: 1443425.5831724291\n",
      "Epoch 30 - Loss: 2293070.5141619607\n",
      "Epoch 31 - Loss: 3642842.798114249\n",
      "Epoch 32 - Loss: 5787132.901551378\n",
      "Epoch 33 - Loss: 9193618.647457335\n",
      "Epoch 34 - Loss: 14605267.526210772\n",
      "Epoch 35 - Loss: 23202380.729402818\n",
      "Epoch 36 - Loss: 36860021.32627528\n",
      "Epoch 37 - Loss: 58556972.64659321\n",
      "Epoch 38 - Loss: 93025422.24778332\n",
      "Epoch 39 - Loss: 147783070.00914103\n",
      "Epoch 40 - Loss: 234772767.05168927\n",
      "Epoch 41 - Loss: 372967296.96468127\n",
      "Epoch 42 - Loss: 592507412.0255105\n",
      "Epoch 43 - Loss: 941275645.8420335\n",
      "Epoch 44 - Loss: 1495339675.9428892\n",
      "Epoch 45 - Loss: 2375542973.3939266\n",
      "Epoch 46 - Loss: 3773861223.186244\n",
      "Epoch 47 - Loss: 5995272950.868138\n",
      "Epoch 48 - Loss: 9524276498.247444\n",
      "Epoch 49 - Loss: 15130560953.408405\n",
      "Epoch 50 - Loss: 24036878266.554638\n",
      "Epoch 51 - Loss: 38185730098.256775\n",
      "Epoch 52 - Loss: 60663034815.43407\n",
      "Epoch 53 - Loss: 96371178017.33083\n",
      "Epoch 54 - Loss: 153098241469.59363\n",
      "Epoch 55 - Loss: 243216613341.3103\n",
      "Epoch 56 - Loss: 386381453094.4433\n",
      "Epoch 57 - Loss: 613817556475.4111\n",
      "Epoch 58 - Loss: 975129602158.6553\n",
      "Epoch 59 - Loss: 1549121120722.1724\n",
      "Epoch 60 - Loss: 2460981844213.489\n",
      "Epoch 61 - Loss: 3909592062578.7627\n",
      "Epoch 62 - Loss: 6210899170881.103\n",
      "Epoch 63 - Loss: 9866826997138.828\n",
      "Epoch 64 - Loss: 15674747297122.348\n",
      "Epoch 65 - Loss: 24901389565246.473\n",
      "Epoch 66 - Loss: 39559119552377.31\n",
      "Epoch 67 - Loss: 62844843885473.94\n",
      "Epoch 68 - Loss: 99837267554966.34\n",
      "Epoch 69 - Loss: 158604578778273.22\n",
      "Epoch 70 - Loss: 251964151518709.62\n",
      "Epoch 71 - Loss: 400278063468114.7\n",
      "Epoch 72 - Loss: 635894142591497.8\n",
      "Epoch 73 - Loss: 1010201151366334.1\n",
      "Epoch 74 - Loss: 1604836871217174.5\n",
      "Epoch 75 - Loss: 2549493612964774.5\n",
      "Epoch 76 - Loss: 4050204602800765.5\n",
      "Epoch 77 - Loss: 6434280612090774.0\n",
      "Epoch 78 - Loss: 1.0221697680778552e+16\n",
      "Epoch 79 - Loss: 1.6238505868223644e+16\n",
      "Epoch 80 - Loss: 2.579699391111806e+16\n",
      "Epoch 81 - Loss: 4.098190438521302e+16\n",
      "Epoch 82 - Loss: 6.510512398558577e+16\n",
      "Epoch 83 - Loss: 1.0342801860393498e+17\n",
      "Epoch 84 - Loss: 1.643089572289932e+17\n",
      "Epoch 85 - Loss: 2.6102630399468957e+17\n",
      "Epoch 86 - Loss: 4.146744798712976e+17\n",
      "Epoch 87 - Loss: 6.587647360628819e+17\n",
      "Epoch 88 - Loss: 1.0465340852774207e+18\n",
      "Epoch 89 - Loss: 1.662556496562231e+18\n",
      "Epoch 90 - Loss: 2.641188799434618e+18\n",
      "Epoch 91 - Loss: 4.1958744191149757e+18\n",
      "Epoch 92 - Loss: 6.665696198905629e+18\n",
      "Epoch 93 - Loss: 1.0589331657232187e+19\n",
      "Epoch 94 - Loss: 1.6822540602025929e+19\n",
      "Epoch 95 - Loss: 2.6724809597736215e+19\n",
      "Epoch 96 - Loss: 4.245586115269898e+19\n",
      "Epoch 97 - Loss: 6.744669740771284e+19\n",
      "Epoch 98 - Loss: 1.071479147448264e+20\n",
      "Epoch 99 - Loss: 1.7021849957699663e+20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 3.17604541e+09, -1.26772695e+10],\n",
       "       [-4.78320510e+09,  1.90922901e+10],\n",
       "       [ 2.75112572e+09, -1.09811913e+10],\n",
       "       [-3.51968871e+09,  1.40489309e+10],\n",
       "       [-8.45596953e+08,  3.37522267e+09],\n",
       "       [-5.47425472e+09,  2.18506330e+10],\n",
       "       [ 2.84981651e+09, -1.13751182e+10],\n",
       "       [-5.07532497e+09,  2.02582943e+10],\n",
       "       [ 3.91897722e+09, -1.56427015e+10],\n",
       "       [-2.90867977e+09,  1.16100725e+10]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X: NDArray = np.random.randn(100, 2)\n",
    "print(X.shape)\n",
    "\n",
    "# We need to unpack the x1 and x2 into a single vector\n",
    "x1: NDArray = X[:, 0]\n",
    "x2: NDArray = X[:, 1]\n",
    "y: NDArray = np.sin(x1) * np.cos(x2) + np.sin(0.5 * x1) * np.cos(0.5 * x2)\n",
    "print(y.shape)\n",
    "\n",
    "# Calculate the derivative of y\n",
    "dy_dx1: NDArray = np.cos(x1) * np.cos(x2) - np.cos(0.5 * x1) * np.cos(0.5 * x2)\n",
    "dy_dx2: NDArray = -np.sin(x1) * np.sin(x2) - 0.5 * np.sin(0.5 * x1) * np.sin(0.5 * x2)\n",
    "\n",
    "parameters = {\n",
    "    \"epochs\": 100,\n",
    "    \"lr\": 0.01,\n",
    "    \"parameters\": np.random.randn(10, 2)\n",
    "}\n",
    "ls_sdg(X, y, parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
